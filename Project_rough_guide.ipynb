{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "# Data reading\n",
    "import pandas as pd\n",
    "import csv\n",
    "import pickle\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# networkx\n",
    "\n",
    "import networkx as nx\n",
    "from networkx.algorithms import community\n",
    "from networkx.algorithms.community import greedy_modularity_communities\n",
    "from networkx.algorithms.community import k_clique_communities\n",
    "\n",
    "from community import community_louvain\n",
    "\n",
    "import scipy.sparse.linalg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Protein-protein interaction networks\n",
    "\n",
    "The protein-protein interaction network of Yeast *saccharomyces cerevisiae* has more than $6,000$ proteins. Proteins form physical complexes of $5$-$50$ proteins and there are roughly $100$-$200$ functional communities. \n",
    "\n",
    "The gene HST1 (*standard name*) is also called YOL068C (*systematic name*)\n",
    "\n",
    "See https://www.yeastgenome.org for details; the data can be dowloaded from https://string-db.org/cgi/network.pl?taskId=nhiAd00Ggrzi\n",
    "\n",
    "Thanks to Eduardo Altmann for sharing his Jupyter notebooks from which we have shamelessly copied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Read in networks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read networks from file\n",
    "#G_BDF1=nx.read_weighted_edgelist(\"./../Data/4932_protein_links_v11_0.txt\",comments=\"#\",nodetype=str)\n",
    "G_BDF1=nx.read_weighted_edgelist(\"4932.protein.links.v11.0.txt\",comments=\"#\",nodetype=str)\n",
    "\n",
    "print('number of nodes of G:',nx.number_of_nodes(G_BDF1))\n",
    "print('number of edges of G:',nx.number_of_edges(G_BDF1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_target = '4932.YLR399C' # BDF1 \n",
    "print('BDF1 has %i links'%G_BDF1.degree(node_target))\n",
    "\n",
    "# Choose the network to be analyzed below\n",
    "G0=G_BDF1\n",
    "\n",
    "highest_degree_node = max(G0.nodes, key=G0.degree)\n",
    "print(\"The node with the maximum degree is: \", highest_degree_node,\n",
    "      \". It has degree: \", G0.degree(highest_degree_node))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the network to be analyzed below\n",
    "G0=G_BDF1\n",
    "\n",
    "print('number of nodes of the full G:',len(G0.nodes))\n",
    "print('number of edges of the full G:',nx.number_of_edges(G0))\n",
    "print('Is the full G connected?',nx.connected.is_connected(G0))\n",
    "print('How many connected subgraphs are there?',nx.connected.number_connected_components(G0))\n",
    "\n",
    "# delete those edges with a combined score of <= thershold_score (small confidence)\n",
    "threshold_score = 700\n",
    "#threshold_score = 0\n",
    "\n",
    "for edge in G0.edges: \n",
    "    G0.get_edge_data(edge[0],edge[1])\n",
    "    weight = list(G0.get_edge_data(edge[0],edge[1]).values())\n",
    "    #print('qwe',weight[0])\n",
    "    if(weight[0] <= threshold_score):\n",
    "        G0.remove_edge(edge[0],edge[1])\n",
    "\n",
    "# restrict to largest connected component\n",
    "largest_cc = max(nx.connected_components(G0),key=len)\n",
    "G0=G0.subgraph(largest_cc)\n",
    "print('number of nodes of restricted G:',len(G0.nodes))\n",
    "print('number of edges of restricted G:',nx.number_of_edges(G0))\n",
    "\n",
    "# randomize according to degree-preserving Maslov-Sneppen algorithm\n",
    "# G0_randomized = nx.random_reference(G0,connectivity=True) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II.  Community detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partToList(part):\n",
    "    '''Print partition as a list'''\n",
    "    N=0\n",
    "    for b in range(len(part)):\n",
    "        N=N+len(part[b])\n",
    "    listP=np.zeros(N)\n",
    "    for b in range(len(part)):\n",
    "        for n in part[b]:\n",
    "            listP[n]=b\n",
    "    return(listP)\n",
    "\n",
    "def partToList_for_strings(part,G):\n",
    "    '''Print partition as a list'''\n",
    "    N=0\n",
    "    for b in range(len(part)):\n",
    "        N=N+len(part[b])\n",
    "    listP=np.zeros(N)\n",
    "    for b in range(len(part)):\n",
    "        for n in part[b]:\n",
    "            ind_n = list(G.nodes).index(n)\n",
    "            listP[ind_n]=b\n",
    "    return(listP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II a. Spectral methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G0_mod = nx.modularity_matrix(G0, nodelist=None, weight=None)\n",
    "\n",
    "# modularity (is numpy.matrix)\n",
    "EVal, EVec = np.linalg.eig(G0_mod)\n",
    "idx = np.argsort(EVal)\n",
    "idx = idx[::-1]\n",
    "EVal_mod = EVal.real[idx]\n",
    "EVec_mod = EVec[:,idx]\n",
    "print('lambda_mod=',EVal_mod[1:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partClub = np.sign(EVec_mod.real[:,0])\n",
    "ind_neg = [i for i in range(len(partClub)) if partClub[i]<0] \n",
    "ind_pos = [i for i in range(len(partClub)) if partClub[i]>0] \n",
    "partClub = (ind_neg,ind_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random walk (almost invariant sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G0_Ad = nx.adjacency_matrix(G0, nodelist=None, weight=None)\n",
    "# adjacency matrix (is scipy.sparse.csr.csr_matrix)\n",
    "\n",
    "G0_D = G0_Ad.sum(axis=1)\n",
    "G0_D = sp.sparse.spdiags(G0_D.flatten(), [0], len(G0.nodes), len(G0.nodes), format='csr')\n",
    "G0_Markov = sp.sparse.linalg.inv(G0_D).dot(G0_Ad) # right eigenvector (expectation values)\n",
    "G0_Markov = G0_Ad.dot(sp.sparse.linalg.inv(G0_D)) # left eigenvector (density)\n",
    "\n",
    "# Markov matrix (is scipy.sparse.csr.csr_matrix)\n",
    "G0_Markov = G0_Markov.astype(float);\n",
    "EVal, EVec = sp.sparse.linalg.eigs(G0_Markov,k=6)\n",
    "idx = np.argsort(EVal)\n",
    "idx = idx[::-1]\n",
    "EVal_Markov = EVal.real[idx]\n",
    "EVec_Markov = EVec[:,idx]\n",
    "print('lambda_Markov=',EVal_Markov.real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partClub = np.sign(EVec_Markov.real[:,1])\n",
    "ind_neg = [i for i in range(len(partClub)) if partClub[i]<0] \n",
    "ind_pos = [i for i in range(len(partClub)) if partClub[i]>0] \n",
    "partClub = (ind_neg,ind_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Laplacian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G0_Lap = nx.laplacian_matrix(G0, nodelist=None, weight=None)\n",
    "\n",
    "# graph Laplacian matrix (is scipy.sparse.csr.csr_matrix)\n",
    "G0_Lap = G0_Lap.astype(float)\n",
    "\n",
    "EVal, EVec = sp.sparse.linalg.eigs(G0_Lap,k=15,which='SR')\n",
    "idx = np.argsort(EVal)\n",
    "#idx = idx[::-1]\n",
    "EVal_Lap = EVal.real[idx]\n",
    "EVec_Lap = EVec[:,idx]\n",
    "print('lambda_Lap=',EVal_Lap)\n",
    "\n",
    "#G0_lap_v2 = nx.fiedler_vector(G0, weight=None, normalized=False, tol=1e-08, seed=None)\n",
    "\n",
    "## compare different eigenvalue routines (don't trust the higher ones)\n",
    "# e = np.linalg.eigvals(G0_Lap.A)\n",
    "# L.A is shorthand for L.toarray()\n",
    "\n",
    "plt.plot(EVal_Lap,\"-d\")\n",
    "plt.xlabel(\"$B$\")\n",
    "plt.ylabel(\"$\\lambda$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partClub = np.sign(EVec_Lap.real[:,1])\n",
    "\n",
    "ind_neg = [i for i in range(len(partClub)) if partClub[i]<0] \n",
    "ind_pos = [i for i in range(len(partClub)) if partClub[i]>0] \n",
    "partClub = (ind_neg,ind_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalized Graph Laplacian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G0_normLap = nx.normalized_laplacian_matrix(G0, nodelist=None, weight=None)\n",
    "\n",
    "# normalized graph Laplacian matrix (is scipy.sparse.csr.csr_matrix)\n",
    "G0_normLap = G0_normLap.astype(float)\n",
    "EVal, EVec = sp.sparse.linalg.eigs(G0_normLap,k=6,which='SR')\n",
    "idx = np.argsort(EVal)\n",
    "#idx = idx[::-1]\n",
    "EVal_normLap = EVal.real[idx]\n",
    "EVec_normLap = EVec[:,idx]\n",
    "print('lambda_normLap=',EVal_normLap)\n",
    "\n",
    "G0_normlap_v2 = nx.fiedler_vector(G0, weight=None, normalized=True, tol=1e-08, seed=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partClub = np.sign(EVec_normLap.real[:,1])\n",
    "\n",
    "ind_neg = [i for i in range(len(partClub)) if partClub[i]<0] \n",
    "ind_pos = [i for i in range(len(partClub)) if partClub[i]>0] \n",
    "partClub = (ind_neg,ind_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjacency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G0_Ad = nx.adjacency_matrix(G0, nodelist=None, weight=None)\n",
    "\n",
    "# adjacency matrix (is scipy.sparse.csr.csr_matrix)\n",
    "G0_Ad = G0_Ad.astype(float)\n",
    "EVal, EVec = sp.sparse.linalg.eigs(G0_Ad,k=6,which='LR')\n",
    "idx = np.argsort(EVal)\n",
    "idx = idx[::-1]\n",
    "EVal_Ad = EVal.real[idx]\n",
    "EVec_Ad = EVec[:,idx]\n",
    "print('lambda_Ad=',EVal_Ad)\n",
    "\n",
    "plt.plot(EVal_Ad,\"-d\")\n",
    "plt.xlabel(\"$B$\")\n",
    "plt.ylabel(\"$\\lambda$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partClub = np.sign(EVec_Ad.real[:,1])\n",
    "ind_neg = [i for i in range(len(partClub)) if partClub[i]<0] \n",
    "ind_pos = [i for i in range(len(partClub)) if partClub[i]>0] \n",
    "partClub = (ind_neg,ind_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II b. Modularity community detection algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Louvain group modularity algorithm\n",
    "\n",
    "!pip3 install python-louvain\n",
    "\n",
    "https://iopscience.iop.org/article/10.1088/1742-5468/2008/10/P10008\n",
    "\n",
    "https://en.wikipedia.org/wiki/Louvain_modularity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partLouvain = community_louvain.best_partition(G0)\n",
    "number_of_communities = max(list(partLouvain.values()))+1\n",
    "print('# of partitions for Louvain modularity =',number_of_communities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### identify the community to which HST1 (=YOL068C) belongs to (cluster starts at 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_target = '4932.YOL068C' # HST1\n",
    "print('The target protein HST1 belongs to community #',partLouvain[node_target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### find the communities to which this community connects to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the communities as separate graphs\n",
    "\n",
    "#subgraph dict with community number as key and subgraph as value\n",
    "G_cluster = {}\n",
    "\n",
    "#node dict with community number as key and node as value\n",
    "nodes = {}\n",
    "for i in range(number_of_communities):\n",
    "    nodes[i] = []\n",
    "\n",
    "for name, community in partLouvain.items():\n",
    "    nodes[community].append(name)\n",
    "    \n",
    "for key in nodes.keys():\n",
    "    G_cluster[key] = G0.subgraph(nodes[key])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### find the communities which have links to the community of the target protein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = {}\n",
    "for i in range(number_of_communities-1):\n",
    "    for j in range(i+1,number_of_communities):\n",
    "        edges[str(i)+str(j)] = 0\n",
    "\n",
    "for i in range(number_of_communities-1):\n",
    "    for node in G_cluster[i].nodes():\n",
    "        for neighbor in G0.neighbors(node):\n",
    "            for j in range(i+1,number_of_communities):\n",
    "                if neighbor in nodes[j]:\n",
    "                    edges[str(i)+str(j)] += 1\n",
    "print(edges) # convention: 'ij' denotes the edge between node i and node j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the communities which have links to the community of the target protein\n",
    "target_cluster = partLouvain[node_target]\n",
    "neighbor_cluster = []\n",
    "for i in range(number_of_communities):\n",
    "    if i < target_cluster:\n",
    "        if edges[str(i)+str(target_cluster)] != 0:\n",
    "            neighbor_cluster.append(i)\n",
    "    if i > target_cluster:\n",
    "        if edges[str(target_cluster)+str(i)] != 0:\n",
    "            neighbor_cluster.append(i)\n",
    "        \n",
    "print(neighbor_cluster)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### find the central nodes for each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_final = {}\n",
    "for i in range(number_of_communities):\n",
    "    top10_final[i] = []\n",
    "    G = G_cluster[i]\n",
    "    measuresNames=[\"Degree\",\"Eigenvector\",\"Katz\",\"Page Rank\",\"Betweeness\"]\n",
    "    if len(G.nodes)>2:\n",
    "        listMeasures = [dict(nx.degree(G)),nx.eigenvector_centrality_numpy(G),nx.katz_centrality_numpy(G),nx.pagerank(G),nx.betweenness_centrality(G)]\n",
    "   \n",
    "    if len(G.nodes)<=2:\n",
    "        listMeasures = [dict(nx.degree(G)),nx.eigenvector_centrality(G),nx.katz_centrality(G),nx.pagerank(G),nx.betweenness_centrality(G)]\n",
    "   \n",
    "    for idx,dictMeasure in enumerate(listMeasures):\n",
    "        top10 = [[dictMeasure[k],k] for k in dictMeasure.keys()] # Choose the 10 largest values\n",
    "        top10.sort(reverse=True)\n",
    "        top10_final[i].append(top10)\n",
    "        print(\"\\n Centrality Measure in Cluster:\",str(i),measuresNames[idx])\n",
    "        for idx,pair in enumerate(top10[:10]):\n",
    "            print(str(idx+1),\": \\t is node \",pair[1],' with value: %.4f \\t' %(pair[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write data file\n",
    "central_nodes = {'COMMUNITY':[], 'RANK':[], 'DEGREE_NODE':[],'DEGREE_VALUE':[], 'EIGENVECTOR_NODE':[],'EIGENVECTOR_VALUE':[], 'KATZ_NODE':[], 'KATZ_VALUE':[], 'PAGERANK_NODE':[], 'PAGERANK_VALUE':[], 'BETWEENESS_NODE':[], 'BETWEENESS_VALUE':[]}\n",
    " \n",
    "for i in range(number_of_communities):\n",
    "    G = G_cluster[i]\n",
    "    #the number of nodes in top10 central nodes\n",
    "    row_number = min(len(G.nodes),10)\n",
    "    central_nodes['COMMUNITY'].extend([i]*row_number)\n",
    "    central_nodes['RANK'].extend(list(range(1,row_number+1)))\n",
    "    \n",
    "    for j in range(row_number):\n",
    "        central_nodes['DEGREE_NODE'].append(top10_final[i][0][j][1][5:])\n",
    "        central_nodes['DEGREE_VALUE'].append(top10_final[i][0][j][0])\n",
    "        central_nodes['EIGENVECTOR_NODE'].append(top10_final[i][1][j][1][5:])\n",
    "        central_nodes['EIGENVECTOR_VALUE'].append(top10_final[i][1][j][0])\n",
    "        central_nodes['KATZ_NODE'].append(top10_final[i][2][j][1][5:])\n",
    "        central_nodes['KATZ_VALUE'].append(top10_final[i][2][j][0])\n",
    "        central_nodes['PAGERANK_NODE'].append(top10_final[i][3][j][1][5:])\n",
    "        central_nodes['PAGERANK_VALUE'].append(top10_final[i][3][j][0])\n",
    "        central_nodes['BETWEENESS_NODE'].append(top10_final[i][4][j][1][5:])\n",
    "        central_nodes['BETWEENESS_VALUE'].append(top10_final[i][4][j][0])\n",
    "\n",
    "df = pd.DataFrame(central_nodes, columns= ['COMMUNITY','RANK', 'DEGREE_NODE','DEGREE_VALUE', 'EIGENVECTOR_NODE','EIGENVECTOR_VALUE', 'KATZ_NODE', 'KATZ_VALUE', 'PAGERANK_NODE', 'PAGERANK_VALUE', 'BETWEENESS_NODE', 'BETWEENESS_VALUE'])\n",
    "df.to_csv(\"HST1_Centrality_Louvain_thresh%s.csv\" %threshold_score, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write data file\n",
    "top10_final = {}\n",
    "for i in range(number_of_communities):\n",
    "    top10_final['COMMUNITY' + str(i)] = []\n",
    "    G = G_cluster[i]\n",
    "    measuresNames=[\"Degree\",\"Eigenvector\",\"Katz\",\"Page Rank\",\"Betweeness\"]\n",
    "    if len(G.nodes)>2:\n",
    "        listMeasures = [dict(nx.degree(G)),nx.eigenvector_centrality_numpy(G),nx.katz_centrality_numpy(G),nx.pagerank(G),nx.betweenness_centrality(G)]\n",
    "   \n",
    "    if len(G.nodes)<=2:\n",
    "        listMeasures = [dict(nx.degree(G)),nx.eigenvector_centrality(G),nx.katz_centrality(G),nx.pagerank(G),nx.betweenness_centrality(G)]\n",
    "   \n",
    "    for idx,dictMeasure in enumerate(listMeasures):\n",
    "        top10 = [[dictMeasure[k],k] for k in dictMeasure.keys()] # Choose the 10 largest values\n",
    "        top10.sort(reverse=True)\n",
    "        for idx,pair in enumerate(top10[:10]):\n",
    "            top10_final['COMMUNITY' + str(i)].append(pair[1])    \n",
    "        if len(G.nodes)< 10:\n",
    "            top10_final['COMMUNITY' + str(i)].extend(['None']*(10-len(G.nodes)))\n",
    "df = pd.DataFrame(top10_final, columns= list(top10_final.keys()))\n",
    "df.to_csv(\"HST1_Centrality_Louvain_thresh%s_singlecolumn.csv\" %threshold_score, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the partition to be analyzed\n",
    "partComm = partLouvain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avenue #1:\n",
    "### restrict analysis to the cluster of the target node and identify the shortest path between the target node and one of the most central nodes of that same community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restrict to community of target node\n",
    "comm_target = partComm[node_target]\n",
    "G_av1 = G_cluster[comm_target]\n",
    "\n",
    "#node_central = '4932.YNL031C'\n",
    "node_central = '4932.YBR136W' # telomere maintenance\n",
    "#node_central = '4932.YBL021C' # metabolism\n",
    "\n",
    "shortest_path_av1 = nx.shortest_path(G_av1, source=node_target, target=node_central)\n",
    "len_shortest_path_av1 = len(list(shortest_path_av1))\n",
    "\n",
    "print('shortest path between the target protein and the specified central node:', shortest_path_av1)\n",
    "#print(list(nx.all_simple_paths(G_av1, source=node_target, target=node_central, cutoff=len_shortest_path_av1+1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VI. Centrality measures "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "$\\large{C_{net}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#G=G_BDF1\n",
    "G = G0\n",
    "G_randomized = nx.random_reference(G,connectivity=True) \n",
    "print('C_net of G:',nx.transitivity(G))\n",
    "print('C_net of randomized G:',nx.transitivity(G_randomized));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster=nx.clustering(G,nodes=G.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('Clustering:',nx.clustering(G))\n",
    "totalC=0\n",
    "cluster=nx.clustering(G,nodes=G.nodes)\n",
    "cluster_keys=list(cluster)\n",
    "cluster_values=list(cluster.values())\n",
    "for n in range(len(G.nodes)):\n",
    "    totalC = totalC+cluster_values[n]\n",
    "print(\"Average clustering of G = \"+str(totalC/nx.number_of_nodes(G)) )\n",
    "\n",
    "totalC=0\n",
    "cluster=nx.clustering(G_randomized,nodes=G_randomized.nodes)\n",
    "cluster_keys=list(cluster)\n",
    "cluster_values=list(cluster.values())\n",
    "for n in range(len(G_randomized.nodes)):\n",
    "    totalC = totalC+cluster_values[n]\n",
    "print(\"Average clustering of randomized G= \"+str(totalC/nx.number_of_nodes(G_randomized)) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dgList=[]\n",
    "#print('asd',G.degree)\n",
    "for i in G.degree():\n",
    "    dgList.append([i[1],i[0]])\n",
    "dgList.sort()\n",
    "dgList.reverse()\n",
    "for j in dgList[:10]:\n",
    "    print(\"Node \"+str(j[1])+\" has degree \"+str(j[0])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortest path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dij=dict(nx.shortest_path_length(G))\n",
    "diList=[]\n",
    "for k in dij:\n",
    "    di=np.average(list(dict(dij[k]).values()))\n",
    "    diList.append([di,k])\n",
    "\n",
    "diList.sort()\n",
    "print(\"Top 10 nodes with shortest <d_i>:\\n\")\n",
    "for i in diList[:10]:\n",
    "    print(\"\\t Node i=\",i[1],\" has <d_i>=\",i[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Betweeness centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.betweenness_centrality(G);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.betweenness_centrality(G_randomized);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.eigenvector_centrality(G);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.eigenvector_centrality(G_randomized);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measuresNames=[\"Degree\",\"Eigenvector\",\"Katz\",\"Page Rank\",\"Betweeness\"]\n",
    "listMeasures = [dict(nx.degree(G)),nx.eigenvector_centrality_numpy(G),nx.katz_centrality_numpy(G),nx.pagerank(G),nx.betweenness_centrality(G)]\n",
    "\n",
    "for idx,dictMeasure in enumerate(listMeasures):\n",
    "    top10 = [[dictMeasure[k],k] for k in dictMeasure.keys()] # Choose the 10 largest values\n",
    "    top10.sort(reverse=True)\n",
    "    print(\"\\n Centrality Measure:\",measuresNames[idx])\n",
    "    for idx,pair in enumerate(top10[:10]):\n",
    "        print(str(idx+1),\": \\t is node \",pair[1],\" with value:\\t\",pair[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measuresNames=[\"Degree\",\"Eigenvector\",\"Katz\",\"Page Rank\",\"Betweeness\"]\n",
    "listMeasures = [dict(nx.degree(G_randomized)),nx.eigenvector_centrality_numpy(G_randomized),nx.katz_centrality_numpy(G_randomized),nx.pagerank(G_randomized),nx.betweenness_centrality(G_randomized)]\n",
    "\n",
    "for idx,dictMeasure in enumerate(listMeasures):\n",
    "    top10 = [[dictMeasure[k],k] for k in dictMeasure.keys()] # Choose the 10 largest values\n",
    "    top10.sort(reverse=True)\n",
    "    print(\"\\n Centrality Measure:\",measuresNames[idx])\n",
    "    for idx,pair in enumerate(top10[:10]):\n",
    "        print(str(idx+1),\": \\t is node \",pair[1],\" with value:\\t\",pair[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
